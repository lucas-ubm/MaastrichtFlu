{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEN3450, Data Analysis 2020 \n",
    "\n",
    "**Kaggle Competition 2020**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "#import your classifiers here\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosing the Maastricht Flu \n",
    "\n",
    "You are given the early data for an outbreak of a dangerous virus originating from a group of primates being kept in a Maastricht biomedical research lab in the basement of Henri-Paul Spaaklaan building, this virus is dubbed the \"Maastricht Flu\".\n",
    "\n",
    "You have the medical records of $n$ number of patients in `flu_train.csv`. There are two general types of patients in the data, flu patients and healthy (this is recorded in the column labeled `flu`, a 0 indicates the absences of the virus and a 1 indicates presence). Notice that the dataset is unbalanced and you can expect a similar imbalance in the testing set.\n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu. Your goal is to catch as many flu patients as possible without misdiagnosing too many healthy patients.\n",
    "\n",
    "**The deliverable:** submit your final solution via Kaggle competition using the `flu_test.csv` data.\n",
    "\n",
    "Maastricht Gemeente will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "Here are some benchmarks for comparison and for expectation management. Notice that because the dataset is unbalanced, we expect that there is going to be a large difference in the accuracy for each class, thus `accuracy` is a metric that might be misleading in this case (see also below). That's why the baselines below are based on the expected accuracy **per class** and also they give you an estimate for the AUROC on all patients in the testing data. This is the score you see in the Kaggle submission as well.\n",
    "\n",
    "**Baseline Model:** \n",
    "- ~50% expected accuracy on healthy patients in training data\n",
    "- ~50% expected accuracy on flu patients in training data\n",
    "- ~50% expected accuracy on healthy patients in testing data (future data, no info on the labels)\n",
    "- ~50% expected accuracy on flu patients in testing data (future data, no info on the labels)\n",
    "- ~50% expected AUROC on all patients in testing data (future data, no info on the labels)\n",
    "\n",
    "**Reasonable Model:** \n",
    "- ~70% expected accuracy on healthy patients in training data\n",
    "- ~55% expected accuracy on flu patients, in training data\n",
    "- ~70% expected accuracy on healthy patients in testing data (future data, no info on the labels, to be checked upon your submission)\n",
    "- ~57% expected accuracy on flu patients, in testing data (future data, no info on the labels, to be checked upon your submission)\n",
    "- ~65% expected AUROC on all patients, in testing data (future data, no info on the labels, to be checked from Kaggle)\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform the benchmarks (they are kind of low, so we won't care much about this)\n",
    "2. your ability to carefully and thoroughly follow the data analysis pipeline\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read the data, clean and explore the data\n",
    "\n",
    "There are a large number of missing values in the data. Nearly all predictors have some degree of missingness. Not all missingness are alike: NaN in the `'pregnancy'` column is meaningful and informative, as patients with NaN's in the pregnancy column are males, where as NaN's in other predictors may appear randomly. \n",
    "\n",
    "\n",
    "**What do you do?:** We make no attempt to interpret the predictors and we make no attempt to model the missing values in the data in any meaningful way. We replace all missing values with 0.\n",
    "\n",
    "However, it would be more complete to look at the data and allow the data to inform your decision on how to address missingness. For columns where NaN values are informative, you might want to treat NaN as a distinct value; You might want to drop predictors with too many missing values and impute the ones with few missing values using a model. There are many acceptable strategies here, as long as the appropriateness of the method in the context of the task and the data is discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Race1</th>\n      <th>Education</th>\n      <th>MaritalStatus</th>\n      <th>HHIncome</th>\n      <th>HHIncomeMid</th>\n      <th>Poverty</th>\n      <th>HomeRooms</th>\n      <th>...</th>\n      <th>AgeRegMarij</th>\n      <th>HardDrugs</th>\n      <th>SexEver</th>\n      <th>SexAge</th>\n      <th>SexNumPartnLife</th>\n      <th>SexNumPartYear</th>\n      <th>SameSex</th>\n      <th>SexOrientation</th>\n      <th>PregnantNow</th>\n      <th>flu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>51624</td>\n      <td>male</td>\n      <td>34</td>\n      <td>White</td>\n      <td>High School</td>\n      <td>Married</td>\n      <td>25000-34999</td>\n      <td>30000.0</td>\n      <td>1.36</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>16.0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>No</td>\n      <td>Heterosexual</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>51630</td>\n      <td>female</td>\n      <td>49</td>\n      <td>White</td>\n      <td>Some College</td>\n      <td>LivePartner</td>\n      <td>35000-44999</td>\n      <td>40000.0</td>\n      <td>1.91</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>12.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>Heterosexual</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51638</td>\n      <td>male</td>\n      <td>9</td>\n      <td>White</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>75000-99999</td>\n      <td>87500.0</td>\n      <td>1.84</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>51646</td>\n      <td>male</td>\n      <td>8</td>\n      <td>White</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>55000-64999</td>\n      <td>60000.0</td>\n      <td>2.33</td>\n      <td>7.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>51647</td>\n      <td>female</td>\n      <td>45</td>\n      <td>White</td>\n      <td>College Grad</td>\n      <td>Married</td>\n      <td>75000-99999</td>\n      <td>87500.0</td>\n      <td>5.00</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>13.0</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>Yes</td>\n      <td>Bisexual</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 72 columns</p>\n</div>",
      "text/plain": "      ID  Gender  Age  Race1     Education MaritalStatus     HHIncome  \\\n0  51624    male   34  White   High School       Married  25000-34999   \n1  51630  female   49  White  Some College   LivePartner  35000-44999   \n2  51638    male    9  White           NaN           NaN  75000-99999   \n3  51646    male    8  White           NaN           NaN  55000-64999   \n4  51647  female   45  White  College Grad       Married  75000-99999   \n\n   HHIncomeMid  Poverty  HomeRooms  ... AgeRegMarij HardDrugs  SexEver  \\\n0      30000.0     1.36        6.0  ...         NaN       Yes      Yes   \n1      40000.0     1.91        5.0  ...         NaN       Yes      Yes   \n2      87500.0     1.84        6.0  ...         NaN       NaN      NaN   \n3      60000.0     2.33        7.0  ...         NaN       NaN      NaN   \n4      87500.0     5.00        6.0  ...         NaN        No      Yes   \n\n   SexAge  SexNumPartnLife  SexNumPartYear  SameSex SexOrientation  \\\n0    16.0              8.0             1.0       No   Heterosexual   \n1    12.0             10.0             1.0      Yes   Heterosexual   \n2     NaN              NaN             NaN      NaN            NaN   \n3     NaN              NaN             NaN      NaN            NaN   \n4    13.0             20.0             0.0      Yes       Bisexual   \n\n  PregnantNow  flu  \n0         NaN    0  \n1         NaN    0  \n2         NaN    0  \n3         NaN    0  \n4         NaN    0  \n\n[5 rows x 72 columns]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train\n",
    "df = pd.read_csv('data/flu_train.csv')\n",
    "df = df[~np.isnan(df['flu'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Race1</th>\n      <th>Education</th>\n      <th>MaritalStatus</th>\n      <th>HHIncome</th>\n      <th>HHIncomeMid</th>\n      <th>Poverty</th>\n      <th>HomeRooms</th>\n      <th>...</th>\n      <th>RegularMarij</th>\n      <th>AgeRegMarij</th>\n      <th>HardDrugs</th>\n      <th>SexEver</th>\n      <th>SexAge</th>\n      <th>SexNumPartnLife</th>\n      <th>SexNumPartYear</th>\n      <th>SameSex</th>\n      <th>SexOrientation</th>\n      <th>PregnantNow</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>51625</td>\n      <td>male</td>\n      <td>4</td>\n      <td>Other</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20000-24999</td>\n      <td>22500.0</td>\n      <td>1.07</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>51678</td>\n      <td>male</td>\n      <td>60</td>\n      <td>White</td>\n      <td>High School</td>\n      <td>Married</td>\n      <td>15000-19999</td>\n      <td>17500.0</td>\n      <td>1.03</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51694</td>\n      <td>male</td>\n      <td>38</td>\n      <td>White</td>\n      <td>Some College</td>\n      <td>Married</td>\n      <td>20000-24999</td>\n      <td>22500.0</td>\n      <td>1.15</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>NaN</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>23.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>No</td>\n      <td>Heterosexual</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>51695</td>\n      <td>male</td>\n      <td>8</td>\n      <td>White</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>65000-74999</td>\n      <td>70000.0</td>\n      <td>3.55</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>51711</td>\n      <td>female</td>\n      <td>59</td>\n      <td>Other</td>\n      <td>8th Grade</td>\n      <td>Widowed</td>\n      <td>20000-24999</td>\n      <td>22500.0</td>\n      <td>1.37</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 71 columns</p>\n</div>",
      "text/plain": "      ID  Gender  Age  Race1     Education MaritalStatus     HHIncome  \\\n0  51625    male    4  Other           NaN           NaN  20000-24999   \n1  51678    male   60  White   High School       Married  15000-19999   \n2  51694    male   38  White  Some College       Married  20000-24999   \n3  51695    male    8  White           NaN           NaN  65000-74999   \n4  51711  female   59  Other     8th Grade       Widowed  20000-24999   \n\n   HHIncomeMid  Poverty  HomeRooms  ... RegularMarij AgeRegMarij  HardDrugs  \\\n0      22500.0     1.07        9.0  ...          NaN         NaN        NaN   \n1      17500.0     1.03        5.0  ...          NaN         NaN         No   \n2      22500.0     1.15        6.0  ...           No         NaN         No   \n3      70000.0     3.55        5.0  ...          NaN         NaN        NaN   \n4      22500.0     1.37        4.0  ...          NaN         NaN        NaN   \n\n   SexEver  SexAge  SexNumPartnLife  SexNumPartYear SameSex SexOrientation  \\\n0      NaN     NaN              NaN             NaN     NaN            NaN   \n1      Yes    20.0              1.0             NaN      No            NaN   \n2      Yes    23.0              1.0             1.0      No   Heterosexual   \n3      NaN     NaN              NaN             NaN     NaN            NaN   \n4      NaN     NaN              NaN             NaN     NaN            NaN   \n\n   PregnantNow  \n0          NaN  \n1          NaN  \n2          NaN  \n3          NaN  \n4          NaN  \n\n[5 rows x 71 columns]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "df_test = pd.read_csv('data/flu_test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "x train shape: (5246, 71)\nx test shape: (1533, 70)\ntrain class 0: 4936, train class 1: 310\n"
    }
   ],
   "source": [
    "#What's up in each set\n",
    "\n",
    "x = df.values[:, :-1]\n",
    "y = df.values[:, -1]\n",
    "\n",
    "x_test = df_test.values[:, :-1]\n",
    "\n",
    "print('x train shape:', x.shape)\n",
    "print('x test shape:', x_test.shape)\n",
    "print('train class 0: {}, train class 1: {}'.format(len(y[y==0]), len(y[y==1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Choice\n",
    "\n",
    "The first task is to decide which classifier to use (from the ones that we learned this block), i.e. which one would best suit our task and our data. Note that our data are heavily unbalanced, thus you need to do some exploration on how different classifiers handle inbalances in the data (we will discuss some of these techniques during week 3 lecture).\n",
    "\n",
    "It would be possible to do brute force model comparison here - i.e. tune all models and compare which does best with respect to various benchmarks. However, it is also reasonable to do a first round of model comparison by running models (with out of the box parameter settings) on the training data and eliminating some models which performed very poorly.\n",
    "\n",
    "Let the best model win!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning, normalization and data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_bin_num(dataframe, feature, bin_feature, bin_size, stat_measure, min_bin=None, max_bin=None, default_val='No'):\n",
    "    if min_bin is None:\n",
    "        min_bin = dataframe[bin_feature].min()\n",
    "    if max_bin is None:\n",
    "        max_bin = dataframe[bin_feature].max()\n",
    "    new_dataframe = dataframe.copy()\n",
    "    df_meancat = pd.DataFrame(columns=['interval', 'stat_measure'])\n",
    "    for num_bin, subset in dataframe.groupby(pd.cut(dataframe[bin_feature], np.arange(min_bin, max_bin+bin_size, bin_size), include_lowest=True)):\n",
    "        if stat_measure is 'mean':\n",
    "            row = [num_bin, subset[feature].mean()]\n",
    "        elif stat_measure is 'mode': \n",
    "            mode_ar = subset[feature].mode().values\n",
    "            if len(mode_ar) > 0:\n",
    "                row = [num_bin, mode_ar[0]]\n",
    "            else:\n",
    "                row = [num_bin, default_val]\n",
    "        else:\n",
    "            raise Exception('Unknown statistical measure: ' + stat_measure)\n",
    "        df_meancat.loc[len(df_meancat)] = row\n",
    "    for index, row_df in dataframe[dataframe[feature].isna()].iterrows():\n",
    "        for _, row_meancat in df_meancat.iterrows():\n",
    "            if row_df[bin_feature] in row_meancat['interval']:\n",
    "                new_dataframe.at[index, feature] = row_meancat['stat_measure']\n",
    "    return new_dataframe\n",
    "\n",
    "\n",
    "def make_dummy_cols(dataframe, column, prefix, drop_dummy):\n",
    "    dummy = pd.get_dummies(dataframe[column], prefix=prefix)\n",
    "    dummy = dummy.drop(columns=prefix+'_'+drop_dummy)\n",
    "    dataframe = pd.concat([dataframe, dummy], axis=1)\n",
    "    dataframe = dataframe.drop(columns=column)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def cleaning(dataframe_raw):\n",
    "    dataframe = dataframe_raw.copy()\n",
    "\n",
    "    dataframe = dataframe.set_index('ID')\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<=13) & (dataframe['Education'].isna()), 'Education'] = 'Lower School/Kindergarten'\n",
    "    dataframe.loc[(dataframe['Age']==14) & (dataframe['Education'].isna()), 'Education'] = '8th Grade'\n",
    "    dataframe.loc[(dataframe['Age']<=17) & (dataframe['Education'].isna()), 'Education'] = '9 - 11th Grade'\n",
    "    dataframe.loc[(dataframe['Age']<=21) & (dataframe['Education'].isna()), 'Education'] = 'High School'\n",
    "    dataframe['Education'] = dataframe['Education'].fillna('Some College')\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<=20) & (dataframe['MaritalStatus'].isna()), 'MaritalStatus'] = 'NeverMarried'\n",
    "    dataframe.at[dataframe['MaritalStatus'].isna(), 'MaritalStatus'] = fill_bin_num(dataframe, 'MaritalStatus', 'Age', 5, 'mode',20)\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['HHIncome'])\n",
    "\n",
    "    dataframe.loc[dataframe['HHIncomeMid'].isna(), 'HHIncomeMid'] = dataframe['HHIncomeMid'].median()\n",
    "\n",
    "    dataframe.loc[dataframe['Poverty'].isna(), 'Poverty'] = dataframe['Poverty'].median()\n",
    "\n",
    "    dataframe.loc[dataframe['HomeRooms'].isna(), 'HomeRooms'] = dataframe['HomeRooms'].mean()\n",
    "\n",
    "    dataframe.loc[dataframe['HomeOwn'].isna(), 'HomeOwn'] = dataframe['HomeOwn'].mode().values[0]\n",
    "\n",
    "    dataframe.loc[(dataframe['Work'].isna()) & (dataframe['Education'].isna()) & (dataframe['Age']<=20), 'Work'] = 'NotWorking'\n",
    "\n",
    "    dataframe.loc[dataframe['Work'].isna(), 'Work'] = dataframe['Work'].mode().values[0]\n",
    "\n",
    "    dataframe = fill_bin_num(dataframe, 'Weight', 'Age', 2, 'mean')\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['HeadCirc'])\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        if np.isnan(row['Height']) and not np.isnan(row['Length']):\n",
    "            dataframe.at[index, 'Height'] = row['Length']\n",
    "    dataframe = fill_bin_num(dataframe, 'Height', 'Age', 2, 'mean')\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['Length'])\n",
    "\n",
    "    for index, row in dataframe[dataframe['BMI'].isna()].iterrows():\n",
    "        dataframe.at[index, 'BMI'] = row['Weight'] / ((row['Height']/100)**2)\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BMICatUnder20yrs')\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BMI_WHO')\n",
    "\n",
    "    dataframe = fill_bin_num(dataframe, 'Pulse', 'Age', 10, 'mean')\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<10) & (dataframe['BPSysAve'].isna()), 'BPSysAve'] = 105\n",
    "    dataframe = fill_bin_num(dataframe, 'BPSysAve', 'Age', 5, 'mean', 10)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<10) & (dataframe['BPDiaAve'].isna()), 'BPDiaAve'] = 60\n",
    "    dataframe = fill_bin_num(dataframe, 'BPDiaAve', 'Age', 5, 'mean', 10)\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BPSys1')\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BPDia1')\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BPSys2')\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BPDia2')\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BPSys3')\n",
    "\n",
    "    dataframe = dataframe.drop(columns='BPDia3')\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['Testosterone'])\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<10) & (dataframe['DirectChol'].isna()), 'DirectChol'] = 0 \n",
    "    dataframe = fill_bin_num(dataframe, 'DirectChol', 'Age', 5, 'mean', 10)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<10) & (dataframe['TotChol'].isna()), 'TotChol'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'TotChol', 'Age', 5, 'mean', 10)\n",
    "    \n",
    "    dataframe.loc[dataframe['UrineVol1'].isna(), 'UrineVol1'] = dataframe['UrineVol1'].median()\n",
    "\n",
    "    dataframe.loc[dataframe['UrineFlow1'].isna(), 'UrineFlow1'] = dataframe['UrineFlow1'].median()\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['UrineVol2'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['UrineFlow2'])\n",
    "\n",
    "    dataframe['Diabetes'] = dataframe['Diabetes'].fillna('No')\n",
    "\n",
    "    dataframe['DiabetesAge'] = dataframe['DiabetesAge'].fillna(0)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<=12) & (dataframe['HealthGen'].isna()), 'HealthGen'] = 'Good'\n",
    "    dataframe = fill_bin_num(dataframe, 'HealthGen', 'Age', 5, 'mode', 10)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<=12) & (dataframe['DaysMentHlthBad'].isna()), 'DaysMentHlthBad'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'DaysMentHlthBad', 'Age', 5, 'mean', 10)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<=15) & (dataframe['LittleInterest'].isna()), 'LittleInterest'] = 'None'\n",
    "    dataframe = fill_bin_num(dataframe, 'LittleInterest', 'Age', 5, 'mode', 15)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<=12) & (dataframe['DaysMentHlthBad'].isna()), 'DaysMentHlthBad'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'DaysMentHlthBad', 'Age', 5, 'mean', 10)\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        if np.isnan(row['nBabies']) and not np.isnan(row['nPregnancies']):\n",
    "            dataframe.at[index, 'nBabies'] = row['nPregnancies']\n",
    "    dataframe['nBabies'] = dataframe['nBabies'].fillna(0)\n",
    "\n",
    "    dataframe['nPregnancies'] = dataframe['nPregnancies'].fillna(0)\n",
    "\n",
    "    dataframe['Age1stBaby'] = dataframe['Age1stBaby'].fillna(0)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']==0) & (dataframe['SleepHrsNight'].isna()), 'SleepHrsNight'] = 14\n",
    "    dataframe.loc[(dataframe['Age']<=2) & (dataframe['SleepHrsNight'].isna()), 'SleepHrsNight'] = 12\n",
    "    dataframe.loc[(dataframe['Age']<=5) & (dataframe['SleepHrsNight'].isna()), 'SleepHrsNight'] = 10\n",
    "    dataframe.loc[(dataframe['Age']<=10) & (dataframe['SleepHrsNight'].isna()), 'SleepHrsNight'] = 9\n",
    "    dataframe.loc[(dataframe['Age']<=15) & (dataframe['SleepHrsNight'].isna()), 'SleepHrsNight'] = 8\n",
    "    dataframe['SleepHrsNight'] = dataframe['SleepHrsNight'].fillna(dataframe_raw['SleepHrsNight'].mean())\n",
    "\n",
    "    dataframe['SleepTrouble'] = dataframe['SleepTrouble'].fillna('No')\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<=4) & (dataframe['PhysActive'].isna()), 'PhysActive'] = 'No'\n",
    "    dataframe = fill_bin_num(dataframe, 'PhysActive', 'Age', 2, 'mode', 16)\n",
    "    dataframe['PhysActive'] = dataframe['PhysActive'].fillna('Yes') # Big assumption here. All kids between 4 and 16 are physically active\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['PhysActiveDays'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['TVHrsDay'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['TVHrsDayChild'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['CompHrsDay'])\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['CompHrsDayChild'])\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<18) & (dataframe['Alcohol12PlusYr'].isna()), 'Alcohol12PlusYr'] = 'No'\n",
    "    dataframe = fill_bin_num(dataframe, 'Alcohol12PlusYr', 'Age', 5, 'mode', 18)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<18) & (dataframe['AlcoholDay'].isna()), 'AlcoholDay'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'AlcoholDay', 'Age', 5, 'mean', 18)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<18) & (dataframe['AlcoholYear'].isna()), 'AlcoholYear'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'AlcoholYear', 'Age', 5, 'mean', 18)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<20) & (dataframe['SmokeNow'].isna()), 'SmokeNow'] = 'No'\n",
    "    dataframe = fill_bin_num(dataframe, 'SmokeNow', 'Age', 5, 'mode', 20)\n",
    "\n",
    "    dataframe['Smoke100'] = dataframe['Smoke100'].fillna('No')\n",
    "\n",
    "    dataframe['Smoke100n'] = dataframe['Smoke100n'].fillna('No')\n",
    "\n",
    "    dataframe.loc[(dataframe['SmokeNow']=='No') & (dataframe['SmokeAge'].isna()), 'SmokeAge'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'SmokeAge', 'Age', 5, 'mean', 20)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<18) & (dataframe['Marijuana'].isna()), 'Marijuana'] = 'No'\n",
    "    dataframe.loc[(dataframe['Marijuana'].isna()) & (dataframe['SmokeNow']=='No'), 'Marijuana'] = 'No'\n",
    "    dataframe = fill_bin_num(dataframe, 'Marijuana', 'Age', 5, 'mode', 20)\n",
    "\n",
    "    dataframe.loc[(dataframe['Marijuana']=='No') & (dataframe['AgeFirstMarij'].isna()), 'AgeFirstMarij'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'AgeFirstMarij', 'Age', 5, 'mean', 20)\n",
    "\n",
    "    dataframe.loc[(dataframe['Marijuana']=='No') & (dataframe['RegularMarij'].isna()), 'RegularMarij'] = 'No'\n",
    "    dataframe = fill_bin_num(dataframe, 'RegularMarij', 'Age', 5, 'mode', 20)\n",
    "\n",
    "    dataframe.loc[(dataframe['RegularMarij']=='No') & (dataframe['AgeRegMarij'].isna()), 'AgeRegMarij'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'AgeRegMarij', 'Age', 5, 'mean', 20)\n",
    "\n",
    "    dataframe.loc[(dataframe['Age']<18) & (dataframe['HardDrugs'].isna()), 'HardDrugs'] = 'No'\n",
    "    dataframe = fill_bin_num(dataframe, 'HardDrugs', 'Age', 5, 'mode', 18)\n",
    "\n",
    "    mode_sex_age = dataframe['SexAge'].mode()[0]\n",
    "    dataframe.loc[(dataframe['Age']<=mode_sex_age) & (dataframe['SexEver'].isna()), 'SexEver'] = 'No'\n",
    "    dataframe['SexEver'] = dataframe['SexEver'].fillna('Yes')\n",
    "\n",
    "    dataframe.loc[(dataframe['SexEver']=='No') & (dataframe['SexAge'].isna()), 'SexAge'] = 0\n",
    "    dataframe['SexAge'] = dataframe['SexAge'].fillna(mode_sex_age)\n",
    "\n",
    "    dataframe.loc[(dataframe['SexEver']=='No') & (dataframe['SexNumPartnLife'].isna()), 'SexNumPartnLife'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'SexNumPartnLife', 'Age', 5, 'mean')\n",
    "    dataframe['SexNumPartnLife'] = dataframe_raw.loc[(dataframe_raw['Age'] >= 60) & (dataframe_raw['Age'] <= 70), 'SexNumPartnLife'].mode()[0] # Missing values for the elderly. Assumed that lifetime sex partners do not increase after 60.\n",
    "\n",
    "    dataframe.loc[(dataframe['SexEver']=='No') & (dataframe['SexNumPartYear'].isna()), 'SexNumPartYear'] = 0\n",
    "    dataframe = fill_bin_num(dataframe, 'SexNumPartYear', 'Age', 10, 'mean')\n",
    "    dataframe['SexNumPartYear'] = dataframe['SexNumPartYear'].fillna(0)\n",
    "\n",
    "    dataframe['SameSex'] = dataframe['SameSex'].fillna('No')\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['SexOrientation'])\n",
    "\n",
    "    dataframe['PregnantNow'] = dataframe['PregnantNow'].fillna('No')\n",
    "\n",
    "\n",
    "    # Making dummy variables\n",
    "    dataframe['male'] = 1*(dataframe['Gender'] ==  'male')\n",
    "    dataframe = dataframe.drop(columns=['Gender'])\n",
    "\n",
    "    dataframe['white'] = np.where(dataframe['Race1'] == 'white',1,0)\n",
    "    dataframe = dataframe.drop(columns=['Race1'])\n",
    "\n",
    "    dataframe = make_dummy_cols(dataframe, 'Education', 'education', '8th Grade')\n",
    "\n",
    "    dataframe = make_dummy_cols(dataframe, 'MaritalStatus', 'maritalstatus', 'Separated')\n",
    "\n",
    "    dataframe = make_dummy_cols(dataframe, 'HomeOwn', 'homeown', 'Other')\n",
    "\n",
    "    dataframe = make_dummy_cols(dataframe, 'Work', 'work', 'Looking')\n",
    "\n",
    "    dataframe['Diabetes'] = np.where(dataframe['Diabetes'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe = make_dummy_cols(dataframe, 'HealthGen', 'healthgen', 'Poor')\n",
    "\n",
    "    dataframe = make_dummy_cols(dataframe, 'LittleInterest', 'littleinterest', 'None')\n",
    "\n",
    "    dataframe = make_dummy_cols(dataframe, 'Depressed', 'depressed', 'None')\n",
    "\n",
    "    dataframe['SleepTrouble'] = np.where(dataframe['SleepTrouble'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['PhysActive'] = np.where(dataframe['PhysActive'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['Alcohol12PlusYr'] = np.where(dataframe['Alcohol12PlusYr'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['SmokeNow'] = np.where(dataframe['SmokeNow'] == 'Yes',1,0)\n",
    "    \n",
    "    dataframe['Smoke100'] = np.where(dataframe['Smoke100'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['Smoke100n'] = np.where(dataframe['Smoke100n'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['Marijuana'] = np.where(dataframe['Marijuana'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['RegularMarij'] = np.where(dataframe['RegularMarij'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['HardDrugs'] = np.where(dataframe['HardDrugs'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['SexEver'] = np.where(dataframe['SexEver'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['SameSex'] = np.where(dataframe['SameSex'] == 'Yes',1,0)\n",
    "\n",
    "    dataframe['PregnantNow'] = np.where(dataframe['PregnantNow'] == 'Yes',1,0)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "data = cleaning(df).select_dtypes(include = 'number')\n",
    "norm = preprocessing.MinMaxScaler()\n",
    "data_n = norm.fit_transform(data.drop('flu', axis=1))\n",
    "ndata = pd.DataFrame(norm.fit_transform(data.drop('flu', axis=1)), index=data.index)\n",
    "ndata['flu'] = data['flu']\n",
    "num_test = cleaning(df_test).select_dtypes(include='number')\n",
    "ntest = pd.DataFrame(norm.fit_transform(num_test), index=num_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ndata, stratify=ndata['flu'], test_size=0.1)\n",
    "\n",
    "X_train = train.drop('flu', axis=1)\n",
    "X_test = test.drop('flu', axis=1)\n",
    "y_train = train['flu']\n",
    "y_test = test['flu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "cw = []\n",
    "for i in np.linspace(start = 0.006, stop = 0.08, num = 5):\n",
    "    cw.append({0:i, 1:1-i})\n",
    "cw.append('balanced')\n",
    "C = [x for x in np.linspace(start = 0.2, stop = 1.5, num = 5)]\n",
    "C.append(1)\n",
    "\n",
    "param_grid = {\n",
    "    'C':C,\n",
    "    'kernel':['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'degree':[2,3,4,5,6,7,8],\n",
    "    'gamma':['auto'],\n",
    "    'shrinking':[True, False],\n",
    "    'class_weight': cw\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SVC()\n",
    "sv_r = RandomizedSearchCV(sv, param_grid, scoring=scorel, cv=3, return_train_score=True, verbose=2, random_state=42, n_jobs=-2, n_iter=300)\n",
    "sv_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sv_r.best_params_\n",
    "print('The best parameters are {} giving an average Balanced Accuract of {:.4f}'.format(params, sv_r.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(sv_r.best_estimator_.predict(ntest))\n",
    "result = pd.DataFrame(np.array([num_test.index, a], dtype=np.int32).T, columns=['ID', 'Prediction'])\n",
    "result.to_csv('result_svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram \n",
    "import json\n",
    "import os\n",
    "def notify_me(message='Done'):\n",
    "    filename = os.environ['HOME']+'/.telegram'\n",
    "    with open(filename) as f:\n",
    "        json_blob = f.read()\n",
    "        credentials = json.loads(json_blob)\n",
    "    bot = telegram.Bot(token=credentials['api_key'])\n",
    "    bot.send_message(chat_id=credentials['chat_id'], text=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cw = []\n",
    "for i in np.linspace(start = 0.001, stop = 0.4, num = 20):\n",
    "    cw.append({0:i, 1:1-i})\n",
    "cw.append('balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 0.0599\n",
    "param_grid = {\n",
    "    'C':[x for x in np.linspace(start = 0.001, stop = 20, num = 40)],\n",
    "    'penalty':['l1', 'l2', 'elasticnet'],\n",
    "    'max_iter':[10, 100, 1000, 10000],\n",
    "    'class_weight': cw\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=3, error_score=nan,\n             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                          fit_intercept=True,\n                                          intercept_scaling=1, l1_ratio=None,\n                                          max_iter=100, multi_class='auto',\n                                          n_jobs=None, penalty='l2',\n                                          random_state=None, solver='lbfgs',\n                                          tol=0.0001, verbose=0,\n                                          warm_start=False),\n             iid='deprecated', n_jobs=-1,\n             param_grid={'C': [0.001, 0.5137948717948717, 1...\n                                          {0: 0.274, 1: 0.726},\n                                          {0: 0.29500000000000004, 1: 0.705},\n                                          {0: 0.316, 1: 0.6839999999999999},\n                                          {0: 0.337, 1: 0.663},\n                                          {0: 0.35800000000000004,\n                                           1: 0.6419999999999999},\n                                          {0: 0.379, 1: 0.621},\n                                          {0: 0.4, 1: 0.6}, 'balanced'],\n                         'max_iter': [10, 100, 1000, 10000],\n                         'penalty': ['l1', 'l2', 'elasticnet']},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n             scoring='balanced_accuracy', verbose=0)"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_r = GridSearchCV(lr, param_grid, scoring='balanced_accuracy', cv=3, return_train_score=True, verbose=0, n_jobs=-1)\n",
    "lr_r.fit(ndata.drop('flu', axis=1), ndata['flu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = lr_r.best_params_\n",
    "\n",
    "notify_me('The best parameters are {} giving an average ROC AUC score of {:.4f}'.format(params, lr_r.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>param_class_weight</th>\n      <th>param_max_iter</th>\n      <th>param_penalty</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>748</th>\n      <td>0.107531</td>\n      <td>0.007842</td>\n      <td>0.004651</td>\n      <td>0.000961</td>\n      <td>1.02659</td>\n      <td>balanced</td>\n      <td>100</td>\n      <td>l2</td>\n      <td>{'C': 1.0265897435897433, 'class_weight': 'bal...</td>\n      <td>0.658212</td>\n      <td>0.665610</td>\n      <td>0.728881</td>\n      <td>0.684234</td>\n      <td>0.031714</td>\n      <td>1</td>\n      <td>0.728949</td>\n      <td>0.734793</td>\n      <td>0.703191</td>\n      <td>0.722311</td>\n      <td>0.013729</td>\n    </tr>\n    <tr>\n      <th>751</th>\n      <td>0.135089</td>\n      <td>0.006675</td>\n      <td>0.003793</td>\n      <td>0.000261</td>\n      <td>1.02659</td>\n      <td>balanced</td>\n      <td>1000</td>\n      <td>l2</td>\n      <td>{'C': 1.0265897435897433, 'class_weight': 'bal...</td>\n      <td>0.658212</td>\n      <td>0.665306</td>\n      <td>0.728881</td>\n      <td>0.684133</td>\n      <td>0.031774</td>\n      <td>2</td>\n      <td>0.728949</td>\n      <td>0.734945</td>\n      <td>0.703343</td>\n      <td>0.722412</td>\n      <td>0.013704</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>0.141895</td>\n      <td>0.014802</td>\n      <td>0.003962</td>\n      <td>0.000437</td>\n      <td>1.02659</td>\n      <td>balanced</td>\n      <td>10000</td>\n      <td>l2</td>\n      <td>{'C': 1.0265897435897433, 'class_weight': 'bal...</td>\n      <td>0.658212</td>\n      <td>0.665306</td>\n      <td>0.728881</td>\n      <td>0.684133</td>\n      <td>0.031774</td>\n      <td>2</td>\n      <td>0.728949</td>\n      <td>0.734945</td>\n      <td>0.703343</td>\n      <td>0.722412</td>\n      <td>0.013704</td>\n    </tr>\n    <tr>\n      <th>547</th>\n      <td>0.073622</td>\n      <td>0.012917</td>\n      <td>0.005176</td>\n      <td>0.002330</td>\n      <td>1.02659</td>\n      <td>{0: 0.064, 1: 0.9359999999999999}</td>\n      <td>1000</td>\n      <td>l2</td>\n      <td>{'C': 1.0265897435897433, 'class_weight': {0: ...</td>\n      <td>0.643673</td>\n      <td>0.675751</td>\n      <td>0.730723</td>\n      <td>0.683382</td>\n      <td>0.035945</td>\n      <td>4</td>\n      <td>0.726831</td>\n      <td>0.717528</td>\n      <td>0.695613</td>\n      <td>0.713324</td>\n      <td>0.013087</td>\n    </tr>\n    <tr>\n      <th>544</th>\n      <td>0.069095</td>\n      <td>0.004190</td>\n      <td>0.003588</td>\n      <td>0.000091</td>\n      <td>1.02659</td>\n      <td>{0: 0.064, 1: 0.9359999999999999}</td>\n      <td>100</td>\n      <td>l2</td>\n      <td>{'C': 1.0265897435897433, 'class_weight': {0: ...</td>\n      <td>0.643673</td>\n      <td>0.675751</td>\n      <td>0.730723</td>\n      <td>0.683382</td>\n      <td>0.035945</td>\n      <td>4</td>\n      <td>0.726831</td>\n      <td>0.717528</td>\n      <td>0.695613</td>\n      <td>0.713324</td>\n      <td>0.013087</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4020</th>\n      <td>0.001642</td>\n      <td>0.000467</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.69292</td>\n      <td>balanced</td>\n      <td>10</td>\n      <td>l1</td>\n      <td>{'C': 7.692923076923076, 'class_weight': 'bala...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10076</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4019</th>\n      <td>0.003879</td>\n      <td>0.002107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.69292</td>\n      <td>{0: 0.4, 1: 0.6}</td>\n      <td>10000</td>\n      <td>elasticnet</td>\n      <td>{'C': 7.692923076923076, 'class_weight': {0: 0...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10077</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4017</th>\n      <td>0.002412</td>\n      <td>0.000586</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.69292</td>\n      <td>{0: 0.4, 1: 0.6}</td>\n      <td>10000</td>\n      <td>l1</td>\n      <td>{'C': 7.692923076923076, 'class_weight': {0: 0...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10078</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4037</th>\n      <td>0.002332</td>\n      <td>0.000649</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.20572</td>\n      <td>{0: 0.001, 1: 0.999}</td>\n      <td>100</td>\n      <td>elasticnet</td>\n      <td>{'C': 8.205717948717947, 'class_weight': {0: 0...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10079</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10079</th>\n      <td>0.002393</td>\n      <td>0.000458</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>20</td>\n      <td>balanced</td>\n      <td>10000</td>\n      <td>elasticnet</td>\n      <td>{'C': 20.0, 'class_weight': 'balanced', 'max_i...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10080</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10080 rows Ã— 20 columns</p>\n</div>",
      "text/plain": "       mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_C  \\\n748         0.107531      0.007842         0.004651        0.000961  1.02659   \n751         0.135089      0.006675         0.003793        0.000261  1.02659   \n754         0.141895      0.014802         0.003962        0.000437  1.02659   \n547         0.073622      0.012917         0.005176        0.002330  1.02659   \n544         0.069095      0.004190         0.003588        0.000091  1.02659   \n...              ...           ...              ...             ...      ...   \n4020        0.001642      0.000467         0.000000        0.000000  7.69292   \n4019        0.003879      0.002107         0.000000        0.000000  7.69292   \n4017        0.002412      0.000586         0.000000        0.000000  7.69292   \n4037        0.002332      0.000649         0.000000        0.000000  8.20572   \n10079       0.002393      0.000458         0.000000        0.000000       20   \n\n                      param_class_weight param_max_iter param_penalty  \\\n748                             balanced            100            l2   \n751                             balanced           1000            l2   \n754                             balanced          10000            l2   \n547    {0: 0.064, 1: 0.9359999999999999}           1000            l2   \n544    {0: 0.064, 1: 0.9359999999999999}            100            l2   \n...                                  ...            ...           ...   \n4020                            balanced             10            l1   \n4019                    {0: 0.4, 1: 0.6}          10000    elasticnet   \n4017                    {0: 0.4, 1: 0.6}          10000            l1   \n4037                {0: 0.001, 1: 0.999}            100    elasticnet   \n10079                           balanced          10000    elasticnet   \n\n                                                  params  split0_test_score  \\\n748    {'C': 1.0265897435897433, 'class_weight': 'bal...           0.658212   \n751    {'C': 1.0265897435897433, 'class_weight': 'bal...           0.658212   \n754    {'C': 1.0265897435897433, 'class_weight': 'bal...           0.658212   \n547    {'C': 1.0265897435897433, 'class_weight': {0: ...           0.643673   \n544    {'C': 1.0265897435897433, 'class_weight': {0: ...           0.643673   \n...                                                  ...                ...   \n4020   {'C': 7.692923076923076, 'class_weight': 'bala...                NaN   \n4019   {'C': 7.692923076923076, 'class_weight': {0: 0...                NaN   \n4017   {'C': 7.692923076923076, 'class_weight': {0: 0...                NaN   \n4037   {'C': 8.205717948717947, 'class_weight': {0: 0...                NaN   \n10079  {'C': 20.0, 'class_weight': 'balanced', 'max_i...                NaN   \n\n       split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n748             0.665610           0.728881         0.684234        0.031714   \n751             0.665306           0.728881         0.684133        0.031774   \n754             0.665306           0.728881         0.684133        0.031774   \n547             0.675751           0.730723         0.683382        0.035945   \n544             0.675751           0.730723         0.683382        0.035945   \n...                  ...                ...              ...             ...   \n4020                 NaN                NaN              NaN             NaN   \n4019                 NaN                NaN              NaN             NaN   \n4017                 NaN                NaN              NaN             NaN   \n4037                 NaN                NaN              NaN             NaN   \n10079                NaN                NaN              NaN             NaN   \n\n       rank_test_score  split0_train_score  split1_train_score  \\\n748                  1            0.728949            0.734793   \n751                  2            0.728949            0.734945   \n754                  2            0.728949            0.734945   \n547                  4            0.726831            0.717528   \n544                  4            0.726831            0.717528   \n...                ...                 ...                 ...   \n4020             10076                 NaN                 NaN   \n4019             10077                 NaN                 NaN   \n4017             10078                 NaN                 NaN   \n4037             10079                 NaN                 NaN   \n10079            10080                 NaN                 NaN   \n\n       split2_train_score  mean_train_score  std_train_score  \n748              0.703191          0.722311         0.013729  \n751              0.703343          0.722412         0.013704  \n754              0.703343          0.722412         0.013704  \n547              0.695613          0.713324         0.013087  \n544              0.695613          0.713324         0.013087  \n...                   ...               ...              ...  \n4020                  NaN               NaN              NaN  \n4019                  NaN               NaN              NaN  \n4017                  NaN               NaN              NaN  \n4037                  NaN               NaN              NaN  \n10079                 NaN               NaN              NaN  \n\n[10080 rows x 20 columns]"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lr_r.cv_results_).sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.6617766270986633"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 0.064\n",
    "fin = LogisticRegression(class_weight={0:w,1:1-w}, C=2.02, penalty='l2')\n",
    "fin.fit(X_train, y_train)\n",
    "np.mean(cross_val_score(fin, X_train, y_train, scoring='balanced_accuracy', cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = lr_r.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(lr_r.best_estimator_.predict(ntest))\n",
    "result = pd.DataFrame(np.array([num_test.index, a], dtype=np.int32).T, columns=['ID', 'Prediction'])\n",
    "result.to_csv('result_lr_n.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorel(model, X_test, y_test):\n",
    "    return 0.6*np.mean(cross_val_score(model,X_train,y_train,scoring='balanced_accuracy', cv=5))+0.4*score(model,X_test, y_test)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg_c = xgb.XGBClassifier(max_depth=3)\n",
    "param_grid = {\n",
    "    'objective':['reg:squarederror', 'reg:logistic', 'binary:logistic'],\n",
    "    'scale_pos_weight':[20,21,22],\n",
    "    'colsample_bytree':[0.3],\n",
    "    'eval_metric':['aucpr', 'auc', 'mae', 'map'],\n",
    "    'alpha':[5, 10, 20],\n",
    "    'n_estimators': [5, 10, 25, 40, 50, 100, 125],\n",
    "    'learning_rate': [0.05, 0.1, 0.15]\n",
    "}\n",
    "xg_s = GridSearchCV(xg_c, param_grid, scoring='balanced_accuracy', cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_s.fit(ndata.drop('flu', axis=1), ndata['flu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = xg_s.best_params_\n",
    "print('The best parameters are {} giving an average ROC AUC score of {:.4f}'.format(params, xg_s.best_score_))\n",
    "xg = xg_s.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(xg.predict(ntest.values))\n",
    "result = pd.DataFrame(np.array([num_test.index, a], dtype=np.int32).T, columns=['ID', 'Prediction'])\n",
    "result.to_csv('result_xg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "w0 = 0.0599\n",
    "\n",
    "cw = []\n",
    "for i in np.linspace(start = 0.001, stop = 0.15, num = 10):\n",
    "    cw.append({0:i, 1:1-i})\n",
    "cw.append('balanced')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [20,50,70,110, 130, 150, 200],\n",
    "    'max_features' : ['auto', 'sqrt'], \n",
    "    'max_depth':[3, 5, 7, 10, 15, None],\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'min_samples_split' : [2, 3, 5, 7],\n",
    "    'min_samples_leaf' : [2, 3, 5, 7],\n",
    "    'class_weight': cw\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=3, error_score=nan,\n                   estimator=RandomForestClassifier(bootstrap=True,\n                                                    ccp_alpha=0.0,\n                                                    class_weight=None,\n                                                    criterion='gini',\n                                                    max_depth=None,\n                                                    max_features='auto',\n                                                    max_leaf_nodes=None,\n                                                    max_samples=None,\n                                                    min_impurity_decrease=0.0,\n                                                    min_impurity_split=None,\n                                                    min_samples_leaf=1,\n                                                    min_samples_split=2,\n                                                    min_weight_fraction_leaf=0.0,\n                                                    n_estimators=100,\n                                                    n_jobs...\n                                                          1: 0.8665555555555555},\n                                                         {0: 0.15, 1: 0.85},\n                                                         'balanced'],\n                                        'criterion': ['gini', 'entropy'],\n                                        'max_depth': [3, 5, 7, 10, 15, None],\n                                        'max_features': ['auto', 'sqrt'],\n                                        'min_samples_leaf': [2, 3, 5, 7],\n                                        'min_samples_split': [2, 3, 5, 7],\n                                        'n_estimators': [20, 50, 70, 110, 130,\n                                                         150, 200]},\n                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n                   return_train_score=True, scoring='balanced_accuracy',\n                   verbose=0)"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfs = RandomForestClassifier()\n",
    "rfs_random = RandomizedSearchCV(rfs, param_grid, scoring='balanced_accuracy', cv=3, return_train_score=True, random_state=42, n_jobs=-1, n_iter=1000)\n",
    "rfs_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = rfs_random.best_params_\n",
    "notify_me('The best parameters are {} giving an average ROC AUC score of {:.4f}'.format(params, rfs_random.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                       class_weight={0: 0.050666666666666665,\n                                     1: 0.9493333333333334},\n                       criterion='gini', max_depth=3, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=5, min_samples_split=3,\n                       min_weight_fraction_leaf=0.0, n_estimators=150,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**rfs_random.best_params_)\n",
    "rf.fit(ndata.drop('flu', axis=1), ndata['flu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(rf.predict(ntest))\n",
    "result = pd.DataFrame(np.array([num_test.index, a], dtype=np.int32).T, columns=['ID', 'Prediction'])\n",
    "result.to_csv('result_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_features' : ['auto', 'sqrt'], \n",
    "    'max_depth':[3, 4, 5,6, 7, 10, None],\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'min_samples_split' : [2, 3,4, 5, 7],\n",
    "    'min_samples_leaf' : [2, 3,4, 5, 7],\n",
    "    'class_weight': cw\n",
    "}\n",
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_r = RandomizedSearchCV(clf, param_grid, scoring='balanced_accuracy', cv=3, return_train_score=True, verbose=0, n_iter=2000)\n",
    "clf_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = clf_r.best_params_\n",
    "print('The best parameters are {} giving an average ROC AUC score of {:.4f}'.format(params, clf_r.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(clf_r.best_estimator_.predict(ntest))\n",
    "result = pd.DataFrame(np.array([num_test.index, a], dtype=np.int32).T, columns=['ID', 'Prediction'])\n",
    "result.to_csv('result_dt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LR</th>\n      <th>SVM</th>\n      <th>XGBoost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.69452</td>\n      <td>0.69214</td>\n      <td>0.6761</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "        LR      SVM  XGBoost\n0  0.69452  0.69214   0.6761"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'LR':0.69452, 'SVM':0.69214, 'XGBoost':0.67610}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On evaluation\n",
    "\n",
    "### AUROC\n",
    "\n",
    "As mentioned abbove, we will use the accuracy scores for each class and for the whole dataset, as well as the AUROC score from Kaggle platform. You can coimpute AUROC locally (e.g. on your train/validation set) by calling the relevant scikit learn function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AUROC locally\n",
    "\n",
    "#score = roc_auc_score(real_labels, predicted_labels)\n",
    "\n",
    "#real_labels: the ground truth (0 or 1)\n",
    "#predicted_labels: labels predicted by your algorithm (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (per class)\n",
    "\n",
    "Below there is a function that will be handy for your models. It computes the accuracy per-class, based on a model you pass as parameter and a dataset (split to x/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_score(model, x_test, y_test):\n",
    "    overall = 0\n",
    "    class_0 = 0\n",
    "    class_1 = 0\n",
    "    for i in range(100):\n",
    "        sample = np.random.choice(len(x_test), len(x_test))\n",
    "        x_sub_test = x_test[sample]\n",
    "        y_sub_test = y_test[sample]\n",
    "        \n",
    "        overall += model.score(x_sub_test, y_sub_test)\n",
    "        class_0 += model.score(x_sub_test[y_sub_test==0], y_sub_test[y_sub_test==0])\n",
    "        class_1 += model.score(x_sub_test[y_sub_test==1], y_sub_test[y_sub_test==1])\n",
    "\n",
    "    return pd.Series([overall / 100., \n",
    "                      class_0 / 100.,\n",
    "                      class_1 / 100.],\n",
    "                      index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same job as before, but faster?\n",
    "\n",
    "score = lambda model, x_val, y_val: pd.Series([model.score(x_val, y_val), \n",
    "                                                 model.score(x_val[y_val==0], y_val[y_val==0]),\n",
    "                                                 model.score(x_val[y_val==1], y_val[y_val==1])], \n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorel(model, X_test, y_test):\n",
    "    return 0.8*np.mean(cross_val_score(model,X_train,y_train,scoring='balanced_accuracy', cv=2))+0.2*score(model,X_test, y_test)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution extraction for Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you extract your solutions (predictions) in the correct format required by Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Conclusions\n",
    "\n",
    "Highlight at the end of your notebook, which were the top-3 approaches that produced the best scores for you. That is, provide a table with the scores you got (on the AUROC score you get from Kaggle) and make sure that you judge these in relation to your work on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}